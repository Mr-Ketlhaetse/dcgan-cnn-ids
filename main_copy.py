import pandas as pd
import os
from ctgan import CTGAN
from preprocessing import detect_features, remove_null_rows, combine_datasets
from IGTD_Functions import min_max_transform, table_to_image, select_features_by_variation
import torch
from cnn import CNNTransferLearning, TransferLearningWrapper
from dcgan import DCGAN
from torchvision import transforms
from custom_helpers import load_images_from_folder


# CTGAN hyperparameters
CTGAN_EPOCHS = 1  # number of epochs for training the CTGAN
CTGAN_SAMPLES = 10  # number of samples generated by the CTGAN
SYN_RATIO = 0.2  # real to synthetics data ratio for the combined dataset
SAMPLED_DATA_FILE = f"{SYN_RATIO}_sampled.csv"  # the number is indicative of the ratio used to generate the data file

real_data = pd.read_csv('./data/clean/cleaned_ids2018_sampled.csv').iloc[:1000]
# real_data = pd.read_csv('./data/02-14-2018.csv').iloc[:1000]

# Remove null values
data = remove_null_rows(real_data)

# Differentiate continuous and discrete features
continuous_features, discrete_features = detect_features(data)

# Instantiate and fit CTGAN
ctgan = CTGAN(epochs=CTGAN_EPOCHS)
ctgan.fit(real_data, discrete_features)

# Create synthetic data
synthetic_data = ctgan.sample(CTGAN_SAMPLES)

# Preview synthetic data
print(synthetic_data.head(10))

# Combine real and synthetic data with a specified ratio
sample_data, sampled_filepath = combine_datasets(real_data, synthetic_data, SYN_RATIO, SAMPLED_DATA_FILE)

# Convert the data into image representation using IGTD algorithm

num_row = 26  # Number of pixel rows in image representation
num_col = 3  # Number of pixel columns in image representation
num = num_row * num_col  # Number of features to be included for analysis, which is also the total number of
# pixels in image representation

save_image_size = 4  # Size of pictures (in inches) saved during the execution of IGTD algorithm.
max_step = 30000  # The maximum number of iterations to run the IGTD algorithm, if it does not converge.
val_step = 300  # The number of iterations for determining algorithm convergence. If the error reduction rate
# is smaller than a pre-set threshold for val_step itertions, the algorithm converges.

# Import the example data and linearly scale each feature so that its minimum and maximum values are 0 and 1,
# respectively.
# data = pd.read_csv(sampled_filepath, low_memory=False, sep='\t', engine='c', na_values=['na', '-',
# ''], header=0, index_col=0)
data = pd.read_csv(sampled_filepath)
# Select features with large variations across samples
id = select_features_by_variation(data, variation_measure='var', num=num)
data = data.iloc[:, id]
# Perform min-max transformation so that the maximum and minimum values of every feature become 1 and 0, respectively.
norm_data = min_max_transform(data.values)
norm_data = pd.DataFrame(norm_data, columns=data.columns, index=data.index)

# Run the IGTD algorithm using (1) the Euclidean distance for calculating pairwise feature distances and pariwise pixel
# distances and (2) the absolute function for evaluating the difference between the feature distance ranking matrix and
# the pixel distance ranking matrix. Save the result in Test_1 folder.
fea_dist_method = 'Euclidean'
image_dist_method = 'Euclidean'
error = 'abs'
result_dir = 'Results/Table_To_Image_Conversion/Test_1'
os.makedirs(name=result_dir, exist_ok=True)
generated_images = table_to_image(norm_data, [num_row, num_col], fea_dist_method, image_dist_method, save_image_size,
               max_step, val_step, result_dir, error)

# Run the IGTD algorithm using (1) the Pearson correlation coefficient for calculating pairwise feature distances,
# (2) the Manhattan distance for calculating pariwise pixel distances, and (3) the square function for evaluating
# the difference between the feature distance ranking matrix and the pixel distance ranking matrix.
# Save the result in Test_2 folder.

# fea_dist_method = 'Pearson'
# image_dist_method = 'Manhattan'
# error = 'squared'
# norm_data = norm_data.iloc[:, :800]
# result_dir = '../Results/Table_To_Image_Conversion/Test_2'
# os.makedirs(name=result_dir, exist_ok=True)
# generated_images = table_to_image(norm_data, [num_row, num_col], fea_dist_method, image_dist_method, save_image_size,
            #    max_step, val_step, result_dir, error)

# Split the data for training, testing and evaluation
# First load the image data generated by the CTGAN samples from file
# Specify your folder path and optional image transformations
folder_path = 'Results/Table_To_Image_Conversion/Test_1/data'
transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.ToTensor(),
])

# Create the dataset
image_dataset = load_images_from_folder(folder_path, image_type='png', transform=transform)

# Instantiate and train a DCGAN on the combined dataset
# Example usage:
my_dataset = ""  # datasets.CIFAR10  # Set your dataset class dynamically
dcgan = DCGAN(dataset=image_dataset)
dcgan.train()

# Implement a CNN model using the saved DCGAN model
# Load the pre-trained DCGAN generator
pretrained_dcgan_generator = DCGAN.Generator(nz=100, ngf=64, nc=3)  # Assuming same architecture as before
pretrained_dcgan_generator.load_state_dict(torch.load('dcgan_generator_weights.pth'))
pretrained_dcgan_generator.eval()

# Instantiate the TransferLearningWrapper
transfer_wrapper = TransferLearningWrapper(dcgan_generator=pretrained_dcgan_generator, num_classes=2)

# Specify optimizer, criterion, dataset, and dataloader
transform = transforms.Compose([
    transforms.Resize(64),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

# dataset = datasets.CIFAR10(root='./data', download=True, transform=transform)
# dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)

# Train the CNN model using transfer learning
# transfer_wrapper.train(dataloader, num_epochs=5)

# Save the trained CNN model
transfer_wrapper.save_model()

# Evaluate and display results
# transfer_wrapper.evaluate(test_dataloader_cnn)  # Use your testing DataLoader for CNN here

